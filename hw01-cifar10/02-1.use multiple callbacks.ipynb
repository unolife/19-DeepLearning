{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 출처\n",
    "https://appliedmachinelearning.blog/2018/03/24/achieving-90-accuracy-in-object-recognition-task-on-cifar-10-dataset-with-keras-convolutional-neural-networks/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers import Dense, Activation, Flatten, Dropout, BatchNormalization\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, ReduceLROnPlateau\n",
    "from keras import regularizers\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "from six.moves import cPickle\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_schedule(epoch):\n",
    "    lrate = 0.001\n",
    "    if epoch > 60:\n",
    "        lrate = 0.005\n",
    "    if epoch > 120:\n",
    "        lrate = 0.003\n",
    "    if epoch > 180:\n",
    "        lrate = 0.001\n",
    "    return lrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    # Modify path string. use your path which your dataset is in\n",
    "    path =  r'C:\\Users\\strea\\Links\\baseline_code_final (2)'\n",
    "    fpath = os.path.join(path, 'train_data')\n",
    "    \n",
    "    with open(fpath, 'rb') as f:\n",
    "        d = cPickle.load(f, encoding='bytes')\n",
    "    X_train = d['data']\n",
    "    y_train = d['labels']\n",
    "    X_train = X_train.reshape(X_train.shape[0], 3, 32, 32) # 3072개 숫자가 한줄로 이어진 배열 하나를 32x32짜리 배열 3개로 만듦\n",
    "    X_train = X_train.transpose(0, 2, 3, 1)\n",
    "    y_train = np.reshape(y_train, (len(y_train), 1))\n",
    "    return X_train, y_train\n",
    "\n",
    "x_train, y_train = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed 값 설정\n",
    "seed = 0\n",
    "np.random.seed(seed)\n",
    "tf.set_random_seed(seed)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_train, y_train, test_size=0.3, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[131., 136., 169.],\n",
       "       [144., 150., 183.],\n",
       "       [141., 148., 182.],\n",
       "       [143., 150., 184.],\n",
       "       [146., 156., 187.]], dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train[0][0][0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120.98071 64.11213 (28000, 32, 32, 3) (12000, 32, 32, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.15627757, 0.23426592, 0.7489891 ],\n",
       "       [0.3590473 , 0.45263335, 0.96735656],\n",
       "       [0.31225428, 0.421438  , 0.95175886],\n",
       "       [0.34344962, 0.45263335, 0.9829542 ],\n",
       "       [0.39024264, 0.54621935, 1.0297472 ]], dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# z-score : 표준편차를 단위로서 보았을 때 측정치가 평균에서 얼마만큼 일탈하였는가를 보는 것이다. \n",
    "#           개인의 측정치 X와 평균치 M과의 차를 표준편차(SD)로 나눈 수이다.\n",
    "mean = np.mean(x_train, axis=(0,1,2,3))\n",
    "std = np.std(x_train, axis=(0,1,2,3))\n",
    "x_train = (x_train-mean)/(std+1e-7) # 각 원소와 평균과의 차이를 표준편차로 나눔(ppt에 나옴 -> normalized data)\n",
    "x_test = (x_test-mean)/(std+1e-7)\n",
    "print(mean, std, x_train.shape, x_test.shape)\n",
    "x_train[0][0][0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 10\n",
    "y_train = np_utils.to_categorical(y_train, num_classes)\n",
    "y_test = np_utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\envs\\p36\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\envs\\p36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 32, 32, 32)        896       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 32, 32, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 32, 32, 32)        9248      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 32, 32, 32)        128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 16, 16, 64)        18496     \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 16, 16, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 16, 16, 64)        36928     \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 16, 16, 64)        256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 8, 8, 128)         73856     \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 8, 8, 128)         512       \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 8, 8, 128)         147584    \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 8, 8, 128)         512       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                20490     \n",
      "=================================================================\n",
      "Total params: 309,290\n",
      "Trainable params: 308,394\n",
      "Non-trainable params: 896\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "weight_decay = 1e-4\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay), input_shape=x_train.shape[1:])) \n",
    "# 32x32에 3x3짜리 필터로 padding은 사이즈 유지할 만큼 주고 / l2 regularization을 수행한다(오버피팅 방지), 그리고 input shape을 지정해준다\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(32, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Conv2D(64, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(64, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(Conv2D(128, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(128, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data augmentation\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=15,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    horizontal_flip=True,\n",
    "    )\n",
    "datagen.fit(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 저장 폴더 설정\n",
    "MODEL_DIR = './model/'\n",
    "if not os.path.exists(MODEL_DIR):\n",
    "    os.mkdir(MODEL_DIR)\n",
    "modelpath = MODEL_DIR + '{epoch:02d}-{val_loss:.4f}.h5'\n",
    "\n",
    "# Prepare callbacks for model saving and for learning rate adjustment.\n",
    "checkpoint = ModelCheckpoint(filepath=modelpath,\n",
    "                             monitor='val_loss',\n",
    "                             verbose=1,\n",
    "                             save_best_only=True)\n",
    "\n",
    "lr_scheduler = LearningRateScheduler(lr_schedule)\n",
    "\n",
    "lr_reducer = ReduceLROnPlateau(factor=np.sqrt(0.1),\n",
    "                               cooldown=0,\n",
    "                               patience=5,\n",
    "                               min_lr=0.5e-6)\n",
    "\n",
    "callbacks = [checkpoint, lr_reducer, lr_scheduler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\envs\\p36\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/250\n",
      "218/218 [==============================] - 19s 87ms/step - loss: 2.2342 - acc: 0.3512 - val_loss: 1.7044 - val_acc: 0.4796\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.70440, saving model to ./model/01-1.7044.h5\n",
      "Epoch 2/250\n",
      "218/218 [==============================] - 16s 72ms/step - loss: 1.6119 - acc: 0.4854 - val_loss: 1.4233 - val_acc: 0.5458\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.70440 to 1.42333, saving model to ./model/02-1.4233.h5\n",
      "Epoch 3/250\n",
      "218/218 [==============================] - 15s 70ms/step - loss: 1.3486 - acc: 0.5677 - val_loss: 1.5900 - val_acc: 0.5422\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.42333\n",
      "Epoch 4/250\n",
      "218/218 [==============================] - 15s 70ms/step - loss: 1.2082 - acc: 0.6159 - val_loss: 1.0096 - val_acc: 0.6767\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.42333 to 1.00965, saving model to ./model/04-1.0096.h5\n",
      "Epoch 5/250\n",
      "218/218 [==============================] - 15s 69ms/step - loss: 1.1272 - acc: 0.6438 - val_loss: 1.2025 - val_acc: 0.6676\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.00965\n",
      "Epoch 6/250\n",
      "218/218 [==============================] - 15s 68ms/step - loss: 1.0642 - acc: 0.6655 - val_loss: 1.2595 - val_acc: 0.6470\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.00965\n",
      "Epoch 7/250\n",
      "218/218 [==============================] - 15s 69ms/step - loss: 1.0112 - acc: 0.6858 - val_loss: 1.5671 - val_acc: 0.6674\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.00965\n",
      "Epoch 8/250\n",
      "218/218 [==============================] - 15s 70ms/step - loss: 0.9709 - acc: 0.7004 - val_loss: 1.0188 - val_acc: 0.7070\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 1.00965\n",
      "Epoch 9/250\n",
      "218/218 [==============================] - 15s 69ms/step - loss: 0.9378 - acc: 0.7082 - val_loss: 0.9839 - val_acc: 0.7346\n",
      "\n",
      "Epoch 00009: val_loss improved from 1.00965 to 0.98392, saving model to ./model/09-0.9839.h5\n",
      "Epoch 10/250\n",
      "218/218 [==============================] - 15s 71ms/step - loss: 0.8933 - acc: 0.7240 - val_loss: 1.0550 - val_acc: 0.7268\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.98392\n",
      "Epoch 11/250\n",
      "218/218 [==============================] - 15s 69ms/step - loss: 0.8725 - acc: 0.7298 - val_loss: 1.0803 - val_acc: 0.7213\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.98392\n",
      "Epoch 12/250\n",
      "218/218 [==============================] - 15s 68ms/step - loss: 0.8446 - acc: 0.7401 - val_loss: 0.9249 - val_acc: 0.7487\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.98392 to 0.92489, saving model to ./model/12-0.9249.h5\n",
      "Epoch 13/250\n",
      "218/218 [==============================] - 15s 69ms/step - loss: 0.8279 - acc: 0.7494 - val_loss: 0.9577 - val_acc: 0.7609\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.92489\n",
      "Epoch 14/250\n",
      "218/218 [==============================] - 15s 69ms/step - loss: 0.8099 - acc: 0.7508 - val_loss: 0.9467 - val_acc: 0.7517\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.92489\n",
      "Epoch 15/250\n",
      "218/218 [==============================] - 15s 68ms/step - loss: 0.7912 - acc: 0.7587 - val_loss: 0.8059 - val_acc: 0.7631\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.92489 to 0.80594, saving model to ./model/15-0.8059.h5\n",
      "Epoch 16/250\n",
      "218/218 [==============================] - 15s 68ms/step - loss: 0.7846 - acc: 0.7652 - val_loss: 0.8596 - val_acc: 0.7726\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.80594\n",
      "Epoch 17/250\n",
      "218/218 [==============================] - 15s 70ms/step - loss: 0.7794 - acc: 0.7678 - val_loss: 0.8435 - val_acc: 0.7697\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.80594\n",
      "Epoch 18/250\n",
      "218/218 [==============================] - 15s 68ms/step - loss: 0.7621 - acc: 0.7729 - val_loss: 1.2800 - val_acc: 0.7514\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.80594\n",
      "Epoch 19/250\n",
      "218/218 [==============================] - 15s 69ms/step - loss: 0.7516 - acc: 0.7729 - val_loss: 1.0676 - val_acc: 0.7418\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.80594\n",
      "Epoch 20/250\n",
      "218/218 [==============================] - 15s 68ms/step - loss: 0.7456 - acc: 0.7819 - val_loss: 1.1207 - val_acc: 0.7741\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.80594\n",
      "Epoch 21/250\n",
      "218/218 [==============================] - 15s 68ms/step - loss: 0.7346 - acc: 0.7868 - val_loss: 1.4784 - val_acc: 0.7533\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.80594\n",
      "Epoch 22/250\n",
      "218/218 [==============================] - 15s 69ms/step - loss: 0.7245 - acc: 0.7900 - val_loss: 1.0466 - val_acc: 0.7728\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.80594\n",
      "Epoch 23/250\n",
      "218/218 [==============================] - 15s 69ms/step - loss: 0.7195 - acc: 0.7916 - val_loss: 0.6956 - val_acc: 0.8102\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.80594 to 0.69557, saving model to ./model/23-0.6956.h5\n",
      "Epoch 24/250\n",
      "218/218 [==============================] - 15s 68ms/step - loss: 0.7016 - acc: 0.7950 - val_loss: 0.8853 - val_acc: 0.7828\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.69557\n",
      "Epoch 25/250\n",
      "218/218 [==============================] - 15s 69ms/step - loss: 0.7089 - acc: 0.7980 - val_loss: 1.0761 - val_acc: 0.7748\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.69557\n",
      "Epoch 26/250\n",
      "218/218 [==============================] - 15s 69ms/step - loss: 0.6960 - acc: 0.7961 - val_loss: 0.7617 - val_acc: 0.8013\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.69557\n",
      "Epoch 27/250\n",
      "218/218 [==============================] - 15s 68ms/step - loss: 0.6853 - acc: 0.8044 - val_loss: 0.7262 - val_acc: 0.8112\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.69557\n",
      "Epoch 28/250\n",
      "218/218 [==============================] - 15s 71ms/step - loss: 0.6867 - acc: 0.8058 - val_loss: 0.8804 - val_acc: 0.7905\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.69557\n",
      "Epoch 29/250\n",
      "218/218 [==============================] - 15s 70ms/step - loss: 0.6783 - acc: 0.8047 - val_loss: 1.0184 - val_acc: 0.7865\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.69557\n",
      "Epoch 30/250\n",
      "218/218 [==============================] - 15s 70ms/step - loss: 0.6723 - acc: 0.8072 - val_loss: 0.8044 - val_acc: 0.8064\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.69557\n",
      "Epoch 31/250\n",
      "218/218 [==============================] - 15s 70ms/step - loss: 0.6703 - acc: 0.8111 - val_loss: 1.1675 - val_acc: 0.7840\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.69557\n",
      "Epoch 32/250\n",
      "218/218 [==============================] - 15s 69ms/step - loss: 0.6668 - acc: 0.8123 - val_loss: 0.8088 - val_acc: 0.8062\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.69557\n",
      "Epoch 33/250\n",
      "218/218 [==============================] - 15s 70ms/step - loss: 0.6672 - acc: 0.8122 - val_loss: 0.7964 - val_acc: 0.8065\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.69557\n",
      "Epoch 34/250\n",
      "218/218 [==============================] - 15s 69ms/step - loss: 0.6510 - acc: 0.8179 - val_loss: 0.7081 - val_acc: 0.8180\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.69557\n",
      "Epoch 35/250\n",
      "218/218 [==============================] - 15s 70ms/step - loss: 0.6479 - acc: 0.8209 - val_loss: 0.9746 - val_acc: 0.7798\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.69557\n",
      "Epoch 36/250\n",
      "218/218 [==============================] - 15s 70ms/step - loss: 0.6435 - acc: 0.8208 - val_loss: 0.6701 - val_acc: 0.8233\n",
      "\n",
      "Epoch 00036: val_loss improved from 0.69557 to 0.67009, saving model to ./model/36-0.6701.h5\n",
      "Epoch 37/250\n",
      "218/218 [==============================] - 15s 67ms/step - loss: 0.6408 - acc: 0.8232 - val_loss: 0.7519 - val_acc: 0.8175\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.67009\n",
      "Epoch 38/250\n",
      "218/218 [==============================] - 14s 65ms/step - loss: 0.6469 - acc: 0.8222 - val_loss: 0.6715 - val_acc: 0.8265\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.67009\n",
      "Epoch 39/250\n",
      "218/218 [==============================] - 14s 66ms/step - loss: 0.6365 - acc: 0.8212 - val_loss: 0.8110 - val_acc: 0.7980\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.67009\n",
      "Epoch 40/250\n",
      "218/218 [==============================] - 14s 63ms/step - loss: 0.6351 - acc: 0.8246 - val_loss: 0.7862 - val_acc: 0.8047\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.67009\n",
      "Epoch 41/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "218/218 [==============================] - 15s 68ms/step - loss: 0.6254 - acc: 0.8283 - val_loss: 0.8100 - val_acc: 0.8140\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.67009\n",
      "Epoch 42/250\n",
      "218/218 [==============================] - 15s 70ms/step - loss: 0.6200 - acc: 0.8311 - val_loss: 0.7353 - val_acc: 0.8143\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.67009\n",
      "Epoch 43/250\n",
      "218/218 [==============================] - 15s 70ms/step - loss: 0.6339 - acc: 0.8275 - val_loss: 0.6259 - val_acc: 0.8343\n",
      "\n",
      "Epoch 00043: val_loss improved from 0.67009 to 0.62588, saving model to ./model/43-0.6259.h5\n",
      "Epoch 44/250\n",
      "218/218 [==============================] - 15s 70ms/step - loss: 0.6169 - acc: 0.8332 - val_loss: 0.6509 - val_acc: 0.8337\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.62588\n",
      "Epoch 45/250\n",
      "218/218 [==============================] - 15s 71ms/step - loss: 0.6112 - acc: 0.8346 - val_loss: 0.6476 - val_acc: 0.8335\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.62588\n",
      "Epoch 46/250\n",
      "218/218 [==============================] - 15s 69ms/step - loss: 0.6083 - acc: 0.8346 - val_loss: 0.8163 - val_acc: 0.8101\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.62588\n",
      "Epoch 47/250\n",
      "218/218 [==============================] - 15s 70ms/step - loss: 0.6092 - acc: 0.8340 - val_loss: 0.7621 - val_acc: 0.8217\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.62588\n",
      "Epoch 48/250\n",
      "218/218 [==============================] - 15s 69ms/step - loss: 0.6076 - acc: 0.8354 - val_loss: 0.6381 - val_acc: 0.8407\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.62588\n",
      "Epoch 49/250\n",
      "218/218 [==============================] - 15s 68ms/step - loss: 0.5978 - acc: 0.8370 - val_loss: 0.6879 - val_acc: 0.8292\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.62588\n",
      "Epoch 50/250\n",
      "218/218 [==============================] - 15s 69ms/step - loss: 0.6012 - acc: 0.8387 - val_loss: 0.9130 - val_acc: 0.8085\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.62588\n",
      "Epoch 51/250\n",
      "218/218 [==============================] - 15s 70ms/step - loss: 0.5985 - acc: 0.8376 - val_loss: 0.7186 - val_acc: 0.8249\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.62588\n",
      "Epoch 52/250\n",
      "218/218 [==============================] - 15s 71ms/step - loss: 0.5908 - acc: 0.8403 - val_loss: 0.6810 - val_acc: 0.8362\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.62588\n",
      "Epoch 53/250\n",
      "218/218 [==============================] - 15s 70ms/step - loss: 0.5971 - acc: 0.8405 - val_loss: 0.7198 - val_acc: 0.8227\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.62588\n",
      "Epoch 54/250\n",
      "218/218 [==============================] - 15s 69ms/step - loss: 0.5880 - acc: 0.8438 - val_loss: 0.8370 - val_acc: 0.8208\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.62588\n",
      "Epoch 55/250\n",
      "218/218 [==============================] - 15s 70ms/step - loss: 0.5824 - acc: 0.8441 - val_loss: 0.6717 - val_acc: 0.8465\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.62588\n",
      "Epoch 56/250\n",
      "218/218 [==============================] - 15s 70ms/step - loss: 0.5943 - acc: 0.8397 - val_loss: 0.6282 - val_acc: 0.8418\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.62588\n",
      "Epoch 57/250\n",
      "218/218 [==============================] - 15s 70ms/step - loss: 0.5756 - acc: 0.8457 - val_loss: 0.6214 - val_acc: 0.8452\n",
      "\n",
      "Epoch 00057: val_loss improved from 0.62588 to 0.62139, saving model to ./model/57-0.6214.h5\n",
      "Epoch 58/250\n",
      "218/218 [==============================] - 15s 70ms/step - loss: 0.5793 - acc: 0.8456 - val_loss: 0.6412 - val_acc: 0.8438\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.62139\n",
      "Epoch 59/250\n",
      "218/218 [==============================] - 15s 70ms/step - loss: 0.5780 - acc: 0.8448 - val_loss: 0.6417 - val_acc: 0.8471\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.62139\n",
      "Epoch 60/250\n",
      "218/218 [==============================] - 15s 70ms/step - loss: 0.5708 - acc: 0.8483 - val_loss: 0.6974 - val_acc: 0.8326\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.62139\n",
      "Epoch 61/250\n",
      "218/218 [==============================] - 15s 68ms/step - loss: 0.5677 - acc: 0.8483 - val_loss: 0.6178 - val_acc: 0.8421\n",
      "\n",
      "Epoch 00061: val_loss improved from 0.62139 to 0.61781, saving model to ./model/61-0.6178.h5\n",
      "Epoch 62/250\n",
      "218/218 [==============================] - 15s 69ms/step - loss: 0.9804 - acc: 0.7267 - val_loss: 1.4359 - val_acc: 0.6793\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.61781\n",
      "Epoch 63/250\n",
      "218/218 [==============================] - 15s 69ms/step - loss: 0.9938 - acc: 0.7379 - val_loss: 1.0086 - val_acc: 0.7609\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.61781\n",
      "Epoch 64/250\n",
      "218/218 [==============================] - 15s 69ms/step - loss: 0.9904 - acc: 0.7508 - val_loss: 1.2093 - val_acc: 0.7051\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.61781\n",
      "Epoch 65/250\n",
      "218/218 [==============================] - 15s 68ms/step - loss: 0.9940 - acc: 0.7506 - val_loss: 0.9895 - val_acc: 0.7621\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.61781\n",
      "Epoch 66/250\n",
      "218/218 [==============================] - 15s 68ms/step - loss: 0.9864 - acc: 0.7580 - val_loss: 1.0195 - val_acc: 0.7699\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.61781\n",
      "Epoch 67/250\n",
      "218/218 [==============================] - 15s 69ms/step - loss: 0.9771 - acc: 0.7618 - val_loss: 1.0807 - val_acc: 0.7258\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.61781\n",
      "Epoch 68/250\n",
      "218/218 [==============================] - 15s 69ms/step - loss: 0.9684 - acc: 0.7647 - val_loss: 1.1385 - val_acc: 0.7325\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.61781\n",
      "Epoch 69/250\n",
      "218/218 [==============================] - 15s 70ms/step - loss: 0.9745 - acc: 0.7650 - val_loss: 0.9974 - val_acc: 0.7655\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.61781\n",
      "Epoch 70/250\n",
      "218/218 [==============================] - 15s 68ms/step - loss: 0.9624 - acc: 0.7688 - val_loss: 1.0293 - val_acc: 0.7618\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.61781\n",
      "Epoch 71/250\n",
      "218/218 [==============================] - 16s 72ms/step - loss: 0.9591 - acc: 0.7657 - val_loss: 1.0603 - val_acc: 0.7603\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.61781\n",
      "Epoch 72/250\n",
      "218/218 [==============================] - 15s 68ms/step - loss: 0.9552 - acc: 0.7700 - val_loss: 0.9472 - val_acc: 0.7847\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.61781\n",
      "Epoch 73/250\n",
      "218/218 [==============================] - 15s 69ms/step - loss: 0.9479 - acc: 0.7701 - val_loss: 0.9402 - val_acc: 0.7876\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.61781\n",
      "Epoch 74/250\n",
      "218/218 [==============================] - 15s 69ms/step - loss: 0.9503 - acc: 0.7720 - val_loss: 1.0223 - val_acc: 0.7639\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.61781\n",
      "Epoch 75/250\n",
      "218/218 [==============================] - 15s 70ms/step - loss: 0.9359 - acc: 0.7798 - val_loss: 0.8813 - val_acc: 0.8017\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.61781\n",
      "Epoch 76/250\n",
      "218/218 [==============================] - 15s 69ms/step - loss: 0.9374 - acc: 0.7754 - val_loss: 0.9584 - val_acc: 0.7827\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.61781\n",
      "Epoch 77/250\n",
      "218/218 [==============================] - 15s 69ms/step - loss: 0.9456 - acc: 0.7715 - val_loss: 0.9432 - val_acc: 0.7782\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.61781\n",
      "Epoch 78/250\n",
      "218/218 [==============================] - 15s 69ms/step - loss: 0.9331 - acc: 0.7793 - val_loss: 0.9617 - val_acc: 0.7788\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.61781\n",
      "Epoch 79/250\n",
      "218/218 [==============================] - 15s 69ms/step - loss: 0.9413 - acc: 0.7735 - val_loss: 0.8772 - val_acc: 0.7954\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.61781\n",
      "Epoch 80/250\n",
      "218/218 [==============================] - 15s 70ms/step - loss: 0.9341 - acc: 0.7776 - val_loss: 0.9311 - val_acc: 0.7837\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.61781\n",
      "Epoch 81/250\n",
      "218/218 [==============================] - 15s 68ms/step - loss: 0.9182 - acc: 0.7831 - val_loss: 0.9054 - val_acc: 0.7952\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.61781\n",
      "Epoch 82/250\n",
      "218/218 [==============================] - 15s 68ms/step - loss: 0.9303 - acc: 0.7760 - val_loss: 0.9590 - val_acc: 0.7837\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.61781\n",
      "Epoch 83/250\n",
      "218/218 [==============================] - 15s 68ms/step - loss: 0.9229 - acc: 0.7826 - val_loss: 0.9514 - val_acc: 0.7783\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.61781\n",
      "Epoch 84/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "218/218 [==============================] - 15s 70ms/step - loss: 0.9209 - acc: 0.7779 - val_loss: 1.0491 - val_acc: 0.7541\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.61781\n",
      "Epoch 85/250\n",
      "218/218 [==============================] - 15s 70ms/step - loss: 0.9277 - acc: 0.7781 - val_loss: 1.0454 - val_acc: 0.7619\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.61781\n",
      "Epoch 86/250\n",
      "218/218 [==============================] - 15s 70ms/step - loss: 0.9150 - acc: 0.7821 - val_loss: 0.9505 - val_acc: 0.7727\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.61781\n",
      "Epoch 87/250\n",
      "218/218 [==============================] - 15s 67ms/step - loss: 0.9166 - acc: 0.7820 - val_loss: 0.9038 - val_acc: 0.7915\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.61781\n",
      "Epoch 88/250\n",
      "218/218 [==============================] - 15s 70ms/step - loss: 0.9146 - acc: 0.7842 - val_loss: 0.9829 - val_acc: 0.7737\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.61781\n",
      "Epoch 89/250\n",
      "218/218 [==============================] - 15s 70ms/step - loss: 0.9128 - acc: 0.7848 - val_loss: 0.8489 - val_acc: 0.8083\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.61781\n",
      "Epoch 90/250\n",
      "218/218 [==============================] - 15s 67ms/step - loss: 0.9063 - acc: 0.7861 - val_loss: 1.0910 - val_acc: 0.7472\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.61781\n",
      "Epoch 91/250\n",
      "218/218 [==============================] - 15s 69ms/step - loss: 0.9093 - acc: 0.7844 - val_loss: 0.8853 - val_acc: 0.7953\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.61781\n",
      "Epoch 92/250\n",
      "218/218 [==============================] - 15s 69ms/step - loss: 0.9023 - acc: 0.7875 - val_loss: 0.8523 - val_acc: 0.8083\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.61781\n",
      "Epoch 93/250\n",
      "218/218 [==============================] - 15s 68ms/step - loss: 0.9129 - acc: 0.7817 - val_loss: 0.9322 - val_acc: 0.7799\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.61781\n",
      "Epoch 94/250\n",
      "218/218 [==============================] - 15s 70ms/step - loss: 0.9079 - acc: 0.7841 - val_loss: 0.8785 - val_acc: 0.8003\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.61781\n",
      "Epoch 95/250\n",
      "218/218 [==============================] - 15s 69ms/step - loss: 0.9002 - acc: 0.7883 - val_loss: 0.8499 - val_acc: 0.8081\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.61781\n",
      "Epoch 96/250\n",
      "218/218 [==============================] - 15s 68ms/step - loss: 0.8993 - acc: 0.7868 - val_loss: 1.0061 - val_acc: 0.7602\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.61781\n",
      "Epoch 97/250\n",
      "218/218 [==============================] - 15s 69ms/step - loss: 0.9049 - acc: 0.7849 - val_loss: 0.9955 - val_acc: 0.7698\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.61781\n",
      "Epoch 98/250\n",
      "218/218 [==============================] - 15s 68ms/step - loss: 0.8897 - acc: 0.7912 - val_loss: 0.9992 - val_acc: 0.7716\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.61781\n",
      "Epoch 99/250\n",
      "218/218 [==============================] - 15s 68ms/step - loss: 0.8981 - acc: 0.7887 - val_loss: 0.8245 - val_acc: 0.8118\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.61781\n",
      "Epoch 100/250\n",
      "218/218 [==============================] - 15s 69ms/step - loss: 0.8951 - acc: 0.7861 - val_loss: 0.8260 - val_acc: 0.8201\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.61781\n",
      "Epoch 101/250\n",
      "218/218 [==============================] - 15s 67ms/step - loss: 0.8893 - acc: 0.7904 - val_loss: 0.9741 - val_acc: 0.7687\n",
      "\n",
      "Epoch 00101: val_loss did not improve from 0.61781\n",
      "Epoch 102/250\n",
      "218/218 [==============================] - 15s 68ms/step - loss: 0.8964 - acc: 0.7880 - val_loss: 0.8901 - val_acc: 0.7950\n",
      "\n",
      "Epoch 00102: val_loss did not improve from 0.61781\n",
      "Epoch 103/250\n",
      "218/218 [==============================] - 16s 71ms/step - loss: 0.8910 - acc: 0.7888 - val_loss: 0.8429 - val_acc: 0.8094\n",
      "\n",
      "Epoch 00103: val_loss did not improve from 0.61781\n",
      "Epoch 104/250\n",
      "218/218 [==============================] - 15s 71ms/step - loss: 0.8893 - acc: 0.7896 - val_loss: 0.9404 - val_acc: 0.7793\n",
      "\n",
      "Epoch 00104: val_loss did not improve from 0.61781\n",
      "Epoch 105/250\n",
      "218/218 [==============================] - 15s 68ms/step - loss: 0.8897 - acc: 0.7912 - val_loss: 0.8495 - val_acc: 0.8114\n",
      "\n",
      "Epoch 00105: val_loss did not improve from 0.61781\n",
      "Epoch 106/250\n",
      "218/218 [==============================] - 15s 71ms/step - loss: 0.8839 - acc: 0.7932 - val_loss: 0.9766 - val_acc: 0.7778\n",
      "\n",
      "Epoch 00106: val_loss did not improve from 0.61781\n",
      "Epoch 107/250\n",
      "218/218 [==============================] - 15s 70ms/step - loss: 0.8887 - acc: 0.7910 - val_loss: 0.9279 - val_acc: 0.7789\n",
      "\n",
      "Epoch 00107: val_loss did not improve from 0.61781\n",
      "Epoch 108/250\n",
      "218/218 [==============================] - 15s 70ms/step - loss: 0.8861 - acc: 0.7920 - val_loss: 0.8921 - val_acc: 0.7996\n",
      "\n",
      "Epoch 00108: val_loss did not improve from 0.61781\n",
      "Epoch 109/250\n",
      "218/218 [==============================] - 15s 68ms/step - loss: 0.8868 - acc: 0.7904 - val_loss: 1.0887 - val_acc: 0.7416\n",
      "\n",
      "Epoch 00109: val_loss did not improve from 0.61781\n",
      "Epoch 110/250\n",
      "218/218 [==============================] - 15s 68ms/step - loss: 0.8878 - acc: 0.7900 - val_loss: 1.0306 - val_acc: 0.7664\n",
      "\n",
      "Epoch 00110: val_loss did not improve from 0.61781\n",
      "Epoch 111/250\n",
      "218/218 [==============================] - 15s 70ms/step - loss: 0.8828 - acc: 0.7907 - val_loss: 0.8857 - val_acc: 0.7997\n",
      "\n",
      "Epoch 00111: val_loss did not improve from 0.61781\n",
      "Epoch 112/250\n",
      "218/218 [==============================] - 15s 67ms/step - loss: 0.8785 - acc: 0.7929 - val_loss: 0.9103 - val_acc: 0.7968\n",
      "\n",
      "Epoch 00112: val_loss did not improve from 0.61781\n",
      "Epoch 113/250\n",
      "218/218 [==============================] - 16s 71ms/step - loss: 0.8937 - acc: 0.7886 - val_loss: 0.8318 - val_acc: 0.8192\n",
      "\n",
      "Epoch 00113: val_loss did not improve from 0.61781\n",
      "Epoch 114/250\n",
      "218/218 [==============================] - 15s 70ms/step - loss: 0.8793 - acc: 0.7934 - val_loss: 1.0466 - val_acc: 0.7583\n",
      "\n",
      "Epoch 00114: val_loss did not improve from 0.61781\n",
      "Epoch 115/250\n",
      "218/218 [==============================] - 15s 69ms/step - loss: 0.8874 - acc: 0.7878 - val_loss: 0.9377 - val_acc: 0.7908\n",
      "\n",
      "Epoch 00115: val_loss did not improve from 0.61781\n",
      "Epoch 116/250\n",
      "218/218 [==============================] - 15s 70ms/step - loss: 0.8752 - acc: 0.7957 - val_loss: 0.8188 - val_acc: 0.8187\n",
      "\n",
      "Epoch 00116: val_loss did not improve from 0.61781\n",
      "Epoch 117/250\n",
      "218/218 [==============================] - 15s 68ms/step - loss: 0.8792 - acc: 0.7938 - val_loss: 0.8946 - val_acc: 0.7998\n",
      "\n",
      "Epoch 00117: val_loss did not improve from 0.61781\n",
      "Epoch 118/250\n",
      "218/218 [==============================] - 15s 69ms/step - loss: 0.8851 - acc: 0.7916 - val_loss: 1.0631 - val_acc: 0.7548\n",
      "\n",
      "Epoch 00118: val_loss did not improve from 0.61781\n",
      "Epoch 119/250\n",
      "218/218 [==============================] - 15s 69ms/step - loss: 0.8844 - acc: 0.7959 - val_loss: 0.8963 - val_acc: 0.7980\n",
      "\n",
      "Epoch 00119: val_loss did not improve from 0.61781\n",
      "Epoch 120/250\n",
      "218/218 [==============================] - 15s 68ms/step - loss: 0.8773 - acc: 0.7934 - val_loss: 0.9246 - val_acc: 0.7894\n",
      "\n",
      "Epoch 00120: val_loss did not improve from 0.61781\n",
      "Epoch 121/250\n",
      "218/218 [==============================] - 15s 70ms/step - loss: 0.8792 - acc: 0.7925 - val_loss: 0.9563 - val_acc: 0.7689\n",
      "\n",
      "Epoch 00121: val_loss did not improve from 0.61781\n",
      "Epoch 122/250\n",
      "218/218 [==============================] - 15s 69ms/step - loss: 0.7945 - acc: 0.8188 - val_loss: 0.8784 - val_acc: 0.7941\n",
      "\n",
      "Epoch 00122: val_loss did not improve from 0.61781\n",
      "Epoch 123/250\n",
      "218/218 [==============================] - 15s 69ms/step - loss: 0.7752 - acc: 0.8176 - val_loss: 0.7478 - val_acc: 0.8298\n",
      "\n",
      "Epoch 00123: val_loss did not improve from 0.61781\n",
      "Epoch 124/250\n",
      "218/218 [==============================] - 15s 68ms/step - loss: 0.7446 - acc: 0.8271 - val_loss: 0.7824 - val_acc: 0.8196\n",
      "\n",
      "Epoch 00124: val_loss did not improve from 0.61781\n",
      "Epoch 125/250\n",
      "218/218 [==============================] - 15s 67ms/step - loss: 0.7422 - acc: 0.8242 - val_loss: 0.7611 - val_acc: 0.8232\n",
      "\n",
      "Epoch 00125: val_loss did not improve from 0.61781\n",
      "Epoch 126/250\n",
      "218/218 [==============================] - 15s 70ms/step - loss: 0.7326 - acc: 0.8245 - val_loss: 0.7472 - val_acc: 0.8273\n",
      "\n",
      "Epoch 00126: val_loss did not improve from 0.61781\n",
      "Epoch 127/250\n",
      "218/218 [==============================] - 15s 70ms/step - loss: 0.7207 - acc: 0.8309 - val_loss: 0.6606 - val_acc: 0.8538\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00127: val_loss did not improve from 0.61781\n",
      "Epoch 128/250\n",
      "218/218 [==============================] - 15s 68ms/step - loss: 0.7314 - acc: 0.8249 - val_loss: 0.7552 - val_acc: 0.8207\n",
      "\n",
      "Epoch 00128: val_loss did not improve from 0.61781\n",
      "Epoch 129/250\n",
      "218/218 [==============================] - 15s 69ms/step - loss: 0.7160 - acc: 0.8273 - val_loss: 0.8284 - val_acc: 0.7973\n",
      "\n",
      "Epoch 00129: val_loss did not improve from 0.61781\n",
      "Epoch 130/250\n",
      "218/218 [==============================] - 15s 67ms/step - loss: 0.7108 - acc: 0.8296 - val_loss: 0.8693 - val_acc: 0.7927\n",
      "\n",
      "Epoch 00130: val_loss did not improve from 0.61781\n",
      "Epoch 131/250\n",
      "218/218 [==============================] - 15s 70ms/step - loss: 0.7148 - acc: 0.8254 - val_loss: 0.8124 - val_acc: 0.8063\n",
      "\n",
      "Epoch 00131: val_loss did not improve from 0.61781\n",
      "Epoch 132/250\n",
      "218/218 [==============================] - 15s 70ms/step - loss: 0.7122 - acc: 0.8266 - val_loss: 0.7249 - val_acc: 0.8287\n",
      "\n",
      "Epoch 00132: val_loss did not improve from 0.61781\n",
      "Epoch 133/250\n",
      "218/218 [==============================] - 15s 69ms/step - loss: 0.7100 - acc: 0.8280 - val_loss: 0.7802 - val_acc: 0.8140\n",
      "\n",
      "Epoch 00133: val_loss did not improve from 0.61781\n",
      "Epoch 134/250\n",
      "218/218 [==============================] - 15s 68ms/step - loss: 0.7027 - acc: 0.8330 - val_loss: 0.6897 - val_acc: 0.8379\n",
      "\n",
      "Epoch 00134: val_loss did not improve from 0.61781\n",
      "Epoch 135/250\n",
      "218/218 [==============================] - 16s 71ms/step - loss: 0.7169 - acc: 0.8245 - val_loss: 0.7869 - val_acc: 0.8128\n",
      "\n",
      "Epoch 00135: val_loss did not improve from 0.61781\n",
      "Epoch 136/250\n",
      "218/218 [==============================] - 15s 70ms/step - loss: 0.7037 - acc: 0.8313 - val_loss: 0.7744 - val_acc: 0.8231\n",
      "\n",
      "Epoch 00136: val_loss did not improve from 0.61781\n",
      "Epoch 137/250\n",
      "218/218 [==============================] - 15s 69ms/step - loss: 0.7113 - acc: 0.8278 - val_loss: 0.7940 - val_acc: 0.8119\n",
      "\n",
      "Epoch 00137: val_loss did not improve from 0.61781\n",
      "Epoch 138/250\n",
      "218/218 [==============================] - 15s 70ms/step - loss: 0.6959 - acc: 0.8318 - val_loss: 0.8230 - val_acc: 0.8065\n",
      "\n",
      "Epoch 00138: val_loss did not improve from 0.61781\n",
      "Epoch 139/250\n",
      "218/218 [==============================] - 15s 69ms/step - loss: 0.7035 - acc: 0.8321 - val_loss: 0.7068 - val_acc: 0.8377\n",
      "\n",
      "Epoch 00139: val_loss did not improve from 0.61781\n",
      "Epoch 140/250\n",
      "218/218 [==============================] - 15s 68ms/step - loss: 0.7049 - acc: 0.8263 - val_loss: 0.7591 - val_acc: 0.8193\n",
      "\n",
      "Epoch 00140: val_loss did not improve from 0.61781\n",
      "Epoch 141/250\n",
      "218/218 [==============================] - 15s 67ms/step - loss: 0.7061 - acc: 0.8307 - val_loss: 0.6845 - val_acc: 0.8373\n",
      "\n",
      "Epoch 00141: val_loss did not improve from 0.61781\n",
      "Epoch 142/250\n",
      "218/218 [==============================] - 15s 70ms/step - loss: 0.6931 - acc: 0.8326 - val_loss: 0.7079 - val_acc: 0.8352\n",
      "\n",
      "Epoch 00142: val_loss did not improve from 0.61781\n",
      "Epoch 143/250\n",
      "218/218 [==============================] - 15s 69ms/step - loss: 0.6983 - acc: 0.8301 - val_loss: 0.7584 - val_acc: 0.8234\n",
      "\n",
      "Epoch 00143: val_loss did not improve from 0.61781\n",
      "Epoch 144/250\n",
      "218/218 [==============================] - 15s 69ms/step - loss: 0.7077 - acc: 0.8301 - val_loss: 0.8013 - val_acc: 0.8086\n",
      "\n",
      "Epoch 00144: val_loss did not improve from 0.61781\n",
      "Epoch 145/250\n",
      "218/218 [==============================] - 15s 71ms/step - loss: 0.6894 - acc: 0.8330 - val_loss: 0.7278 - val_acc: 0.8301\n",
      "\n",
      "Epoch 00145: val_loss did not improve from 0.61781\n",
      "Epoch 146/250\n",
      "218/218 [==============================] - 15s 70ms/step - loss: 0.6980 - acc: 0.8310 - val_loss: 0.7622 - val_acc: 0.8201\n",
      "\n",
      "Epoch 00146: val_loss did not improve from 0.61781\n",
      "Epoch 147/250\n",
      "218/218 [==============================] - 15s 70ms/step - loss: 0.7121 - acc: 0.8249 - val_loss: 0.7460 - val_acc: 0.8218\n",
      "\n",
      "Epoch 00147: val_loss did not improve from 0.61781\n",
      "Epoch 148/250\n",
      "218/218 [==============================] - 15s 69ms/step - loss: 0.6951 - acc: 0.8309 - val_loss: 0.7053 - val_acc: 0.8370\n",
      "\n",
      "Epoch 00148: val_loss did not improve from 0.61781\n",
      "Epoch 149/250\n",
      "218/218 [==============================] - 15s 69ms/step - loss: 0.6971 - acc: 0.8316 - val_loss: 0.8624 - val_acc: 0.8048\n",
      "\n",
      "Epoch 00149: val_loss did not improve from 0.61781\n",
      "Epoch 150/250\n",
      "218/218 [==============================] - 15s 70ms/step - loss: 0.7039 - acc: 0.8291 - val_loss: 0.7877 - val_acc: 0.8073\n",
      "\n",
      "Epoch 00150: val_loss did not improve from 0.61781\n",
      "Epoch 151/250\n",
      "218/218 [==============================] - 15s 71ms/step - loss: 0.6894 - acc: 0.8322 - val_loss: 0.7325 - val_acc: 0.8279\n",
      "\n",
      "Epoch 00151: val_loss did not improve from 0.61781\n",
      "Epoch 152/250\n",
      "218/218 [==============================] - 15s 69ms/step - loss: 0.7005 - acc: 0.8289 - val_loss: 0.7657 - val_acc: 0.8167\n",
      "\n",
      "Epoch 00152: val_loss did not improve from 0.61781\n",
      "Epoch 153/250\n",
      "218/218 [==============================] - 15s 70ms/step - loss: 0.6898 - acc: 0.8335 - val_loss: 0.7176 - val_acc: 0.8274\n",
      "\n",
      "Epoch 00153: val_loss did not improve from 0.61781\n",
      "Epoch 154/250\n",
      "218/218 [==============================] - 15s 69ms/step - loss: 0.6989 - acc: 0.8306 - val_loss: 0.7975 - val_acc: 0.8106\n",
      "\n",
      "Epoch 00154: val_loss did not improve from 0.61781\n",
      "Epoch 155/250\n",
      "218/218 [==============================] - 15s 69ms/step - loss: 0.6951 - acc: 0.8302 - val_loss: 0.7080 - val_acc: 0.8384\n",
      "\n",
      "Epoch 00155: val_loss did not improve from 0.61781\n",
      "Epoch 156/250\n",
      "218/218 [==============================] - 15s 68ms/step - loss: 0.6915 - acc: 0.8339 - val_loss: 0.7924 - val_acc: 0.8177\n",
      "\n",
      "Epoch 00156: val_loss did not improve from 0.61781\n",
      "Epoch 157/250\n",
      "218/218 [==============================] - 15s 69ms/step - loss: 0.6951 - acc: 0.8340 - val_loss: 0.7228 - val_acc: 0.8338\n",
      "\n",
      "Epoch 00157: val_loss did not improve from 0.61781\n",
      "Epoch 158/250\n",
      "218/218 [==============================] - 15s 69ms/step - loss: 0.6956 - acc: 0.8321 - val_loss: 0.7160 - val_acc: 0.8327\n",
      "\n",
      "Epoch 00158: val_loss did not improve from 0.61781\n",
      "Epoch 159/250\n",
      "218/218 [==============================] - 15s 70ms/step - loss: 0.6946 - acc: 0.8337 - val_loss: 0.8418 - val_acc: 0.7988\n",
      "\n",
      "Epoch 00159: val_loss did not improve from 0.61781\n",
      "Epoch 160/250\n",
      "218/218 [==============================] - 15s 69ms/step - loss: 0.6957 - acc: 0.8342 - val_loss: 0.7449 - val_acc: 0.8268\n",
      "\n",
      "Epoch 00160: val_loss did not improve from 0.61781\n",
      "Epoch 161/250\n",
      "218/218 [==============================] - 15s 69ms/step - loss: 0.6969 - acc: 0.8312 - val_loss: 0.6723 - val_acc: 0.8447\n",
      "\n",
      "Epoch 00161: val_loss did not improve from 0.61781\n",
      "Epoch 162/250\n",
      "218/218 [==============================] - 15s 69ms/step - loss: 0.6901 - acc: 0.8318 - val_loss: 0.7752 - val_acc: 0.8173\n",
      "\n",
      "Epoch 00162: val_loss did not improve from 0.61781\n",
      "Epoch 163/250\n",
      "218/218 [==============================] - 15s 70ms/step - loss: 0.6931 - acc: 0.8338 - val_loss: 0.8683 - val_acc: 0.7926\n",
      "\n",
      "Epoch 00163: val_loss did not improve from 0.61781\n",
      "Epoch 164/250\n",
      "218/218 [==============================] - 15s 71ms/step - loss: 0.6873 - acc: 0.8352 - val_loss: 0.7745 - val_acc: 0.8227\n",
      "\n",
      "Epoch 00164: val_loss did not improve from 0.61781\n",
      "Epoch 165/250\n",
      "218/218 [==============================] - 15s 69ms/step - loss: 0.6915 - acc: 0.8358 - val_loss: 0.7348 - val_acc: 0.8283\n",
      "\n",
      "Epoch 00165: val_loss did not improve from 0.61781\n",
      "Epoch 166/250\n",
      "218/218 [==============================] - 15s 69ms/step - loss: 0.6937 - acc: 0.8342 - val_loss: 0.6971 - val_acc: 0.8373\n",
      "\n",
      "Epoch 00166: val_loss did not improve from 0.61781\n",
      "Epoch 167/250\n",
      "218/218 [==============================] - 15s 69ms/step - loss: 0.7001 - acc: 0.8306 - val_loss: 0.7016 - val_acc: 0.8317\n",
      "\n",
      "Epoch 00167: val_loss did not improve from 0.61781\n",
      "Epoch 168/250\n",
      "218/218 [==============================] - 15s 69ms/step - loss: 0.6843 - acc: 0.8369 - val_loss: 0.8218 - val_acc: 0.8177\n",
      "\n",
      "Epoch 00168: val_loss did not improve from 0.61781\n",
      "Epoch 169/250\n",
      "218/218 [==============================] - 15s 67ms/step - loss: 0.6960 - acc: 0.8318 - val_loss: 0.7400 - val_acc: 0.8289\n",
      "\n",
      "Epoch 00169: val_loss did not improve from 0.61781\n",
      "Epoch 170/250\n",
      "218/218 [==============================] - 15s 69ms/step - loss: 0.6934 - acc: 0.8344 - val_loss: 0.7071 - val_acc: 0.8361\n",
      "\n",
      "Epoch 00170: val_loss did not improve from 0.61781\n",
      "Epoch 171/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "218/218 [==============================] - 15s 68ms/step - loss: 0.6902 - acc: 0.8334 - val_loss: 0.7312 - val_acc: 0.8300\n",
      "\n",
      "Epoch 00171: val_loss did not improve from 0.61781\n",
      "Epoch 172/250\n",
      "218/218 [==============================] - 15s 70ms/step - loss: 0.6913 - acc: 0.8347 - val_loss: 0.7416 - val_acc: 0.8270\n",
      "\n",
      "Epoch 00172: val_loss did not improve from 0.61781\n",
      "Epoch 173/250\n",
      "218/218 [==============================] - 15s 68ms/step - loss: 0.6898 - acc: 0.8335 - val_loss: 0.7636 - val_acc: 0.8156\n",
      "\n",
      "Epoch 00173: val_loss did not improve from 0.61781\n",
      "Epoch 174/250\n",
      "218/218 [==============================] - 15s 68ms/step - loss: 0.6916 - acc: 0.8315 - val_loss: 0.7407 - val_acc: 0.8263\n",
      "\n",
      "Epoch 00174: val_loss did not improve from 0.61781\n",
      "Epoch 175/250\n",
      "218/218 [==============================] - 15s 69ms/step - loss: 0.6887 - acc: 0.8333 - val_loss: 0.7166 - val_acc: 0.8308\n",
      "\n",
      "Epoch 00175: val_loss did not improve from 0.61781\n",
      "Epoch 176/250\n",
      "218/218 [==============================] - 15s 68ms/step - loss: 0.6889 - acc: 0.8358 - val_loss: 0.7114 - val_acc: 0.8390\n",
      "\n",
      "Epoch 00176: val_loss did not improve from 0.61781\n",
      "Epoch 177/250\n",
      "218/218 [==============================] - 15s 70ms/step - loss: 0.6886 - acc: 0.8374 - val_loss: 0.7571 - val_acc: 0.8224\n",
      "\n",
      "Epoch 00177: val_loss did not improve from 0.61781\n",
      "Epoch 178/250\n",
      "218/218 [==============================] - 15s 69ms/step - loss: 0.6885 - acc: 0.8343 - val_loss: 0.7208 - val_acc: 0.8291\n",
      "\n",
      "Epoch 00178: val_loss did not improve from 0.61781\n",
      "Epoch 179/250\n",
      "218/218 [==============================] - 100s 458ms/step - loss: 0.6834 - acc: 0.8343 - val_loss: 0.7191 - val_acc: 0.8339\n",
      "\n",
      "Epoch 00179: val_loss did not improve from 0.61781\n",
      "Epoch 180/250\n",
      "218/218 [==============================] - 114s 523ms/step - loss: 0.6929 - acc: 0.8343 - val_loss: 0.7118 - val_acc: 0.8348\n",
      "\n",
      "Epoch 00180: val_loss did not improve from 0.61781\n",
      "Epoch 181/250\n",
      "218/218 [==============================] - 90s 412ms/step - loss: 0.6911 - acc: 0.8345 - val_loss: 0.8112 - val_acc: 0.8059\n",
      "\n",
      "Epoch 00181: val_loss did not improve from 0.61781\n",
      "Epoch 182/250\n",
      "218/218 [==============================] - 120s 552ms/step - loss: 0.6220 - acc: 0.8575 - val_loss: 0.6642 - val_acc: 0.8478\n",
      "\n",
      "Epoch 00182: val_loss did not improve from 0.61781\n",
      "Epoch 183/250\n",
      "218/218 [==============================] - 90s 411ms/step - loss: 0.5816 - acc: 0.8651 - val_loss: 0.6339 - val_acc: 0.8550\n",
      "\n",
      "Epoch 00183: val_loss did not improve from 0.61781\n",
      "Epoch 184/250\n",
      "218/218 [==============================] - 60s 274ms/step - loss: 0.5763 - acc: 0.8674 - val_loss: 0.6301 - val_acc: 0.8560\n",
      "\n",
      "Epoch 00184: val_loss did not improve from 0.61781\n",
      "Epoch 185/250\n",
      "218/218 [==============================] - 15s 69ms/step - loss: 0.5556 - acc: 0.8722 - val_loss: 0.6008 - val_acc: 0.8647\n",
      "\n",
      "Epoch 00185: val_loss improved from 0.61781 to 0.60076, saving model to ./model/185-0.6008.h5\n",
      "Epoch 186/250\n",
      "218/218 [==============================] - 15s 71ms/step - loss: 0.5599 - acc: 0.8689 - val_loss: 0.6425 - val_acc: 0.8512\n",
      "\n",
      "Epoch 00186: val_loss did not improve from 0.60076\n",
      "Epoch 187/250\n",
      "218/218 [==============================] - 15s 71ms/step - loss: 0.5446 - acc: 0.8729 - val_loss: 0.5838 - val_acc: 0.8666\n",
      "\n",
      "Epoch 00187: val_loss improved from 0.60076 to 0.58381, saving model to ./model/187-0.5838.h5\n",
      "Epoch 188/250\n",
      "218/218 [==============================] - 15s 69ms/step - loss: 0.5428 - acc: 0.8714 - val_loss: 0.6082 - val_acc: 0.8587\n",
      "\n",
      "Epoch 00188: val_loss did not improve from 0.58381\n",
      "Epoch 189/250\n",
      "218/218 [==============================] - 15s 70ms/step - loss: 0.5343 - acc: 0.8739 - val_loss: 0.6497 - val_acc: 0.8477\n",
      "\n",
      "Epoch 00189: val_loss did not improve from 0.58381\n",
      "Epoch 190/250\n",
      "218/218 [==============================] - 15s 69ms/step - loss: 0.5250 - acc: 0.8763 - val_loss: 0.5610 - val_acc: 0.8699\n",
      "\n",
      "Epoch 00190: val_loss improved from 0.58381 to 0.56104, saving model to ./model/190-0.5610.h5\n",
      "Epoch 191/250\n",
      "218/218 [==============================] - 14s 64ms/step - loss: 0.5184 - acc: 0.8793 - val_loss: 0.6117 - val_acc: 0.8567\n",
      "\n",
      "Epoch 00191: val_loss did not improve from 0.56104\n",
      "Epoch 192/250\n",
      "218/218 [==============================] - 15s 68ms/step - loss: 0.5140 - acc: 0.8778 - val_loss: 0.5776 - val_acc: 0.8644\n",
      "\n",
      "Epoch 00192: val_loss did not improve from 0.56104\n",
      "Epoch 193/250\n",
      "218/218 [==============================] - 16s 72ms/step - loss: 0.5121 - acc: 0.8787 - val_loss: 0.5843 - val_acc: 0.8595\n",
      "\n",
      "Epoch 00193: val_loss did not improve from 0.56104\n",
      "Epoch 194/250\n",
      "218/218 [==============================] - 15s 71ms/step - loss: 0.5060 - acc: 0.8792 - val_loss: 0.5687 - val_acc: 0.8667\n",
      "\n",
      "Epoch 00194: val_loss did not improve from 0.56104\n",
      "Epoch 195/250\n",
      "218/218 [==============================] - 15s 69ms/step - loss: 0.5091 - acc: 0.8769 - val_loss: 0.5963 - val_acc: 0.8572\n",
      "\n",
      "Epoch 00195: val_loss did not improve from 0.56104\n",
      "Epoch 196/250\n",
      "218/218 [==============================] - 15s 70ms/step - loss: 0.5103 - acc: 0.8766 - val_loss: 0.5605 - val_acc: 0.8688\n",
      "\n",
      "Epoch 00196: val_loss improved from 0.56104 to 0.56052, saving model to ./model/196-0.5605.h5\n",
      "Epoch 197/250\n",
      "218/218 [==============================] - 15s 67ms/step - loss: 0.5050 - acc: 0.8764 - val_loss: 0.5608 - val_acc: 0.8643\n",
      "\n",
      "Epoch 00197: val_loss did not improve from 0.56052\n",
      "Epoch 198/250\n",
      "218/218 [==============================] - 15s 69ms/step - loss: 0.4983 - acc: 0.8787 - val_loss: 0.5781 - val_acc: 0.8567\n",
      "\n",
      "Epoch 00198: val_loss did not improve from 0.56052\n",
      "Epoch 199/250\n",
      "218/218 [==============================] - 15s 69ms/step - loss: 0.4924 - acc: 0.8824 - val_loss: 0.5904 - val_acc: 0.8589\n",
      "\n",
      "Epoch 00199: val_loss did not improve from 0.56052\n",
      "Epoch 200/250\n",
      "218/218 [==============================] - 15s 70ms/step - loss: 0.4945 - acc: 0.8804 - val_loss: 0.5837 - val_acc: 0.8615\n",
      "\n",
      "Epoch 00200: val_loss did not improve from 0.56052\n",
      "Epoch 201/250\n",
      "218/218 [==============================] - 16s 71ms/step - loss: 0.4872 - acc: 0.8815 - val_loss: 0.5890 - val_acc: 0.8587\n",
      "\n",
      "Epoch 00201: val_loss did not improve from 0.56052\n",
      "Epoch 202/250\n",
      "218/218 [==============================] - 15s 70ms/step - loss: 0.4827 - acc: 0.8819 - val_loss: 0.5787 - val_acc: 0.8611\n",
      "\n",
      "Epoch 00202: val_loss did not improve from 0.56052\n",
      "Epoch 203/250\n",
      "218/218 [==============================] - 15s 69ms/step - loss: 0.4921 - acc: 0.8802 - val_loss: 0.5380 - val_acc: 0.8744\n",
      "\n",
      "Epoch 00203: val_loss improved from 0.56052 to 0.53798, saving model to ./model/203-0.5380.h5\n",
      "Epoch 204/250\n",
      "218/218 [==============================] - 15s 69ms/step - loss: 0.4846 - acc: 0.8805 - val_loss: 0.6485 - val_acc: 0.8440\n",
      "\n",
      "Epoch 00204: val_loss did not improve from 0.53798\n",
      "Epoch 205/250\n",
      "218/218 [==============================] - 15s 70ms/step - loss: 0.4800 - acc: 0.8818 - val_loss: 0.6009 - val_acc: 0.8565\n",
      "\n",
      "Epoch 00205: val_loss did not improve from 0.53798\n",
      "Epoch 206/250\n",
      "218/218 [==============================] - 15s 70ms/step - loss: 0.4882 - acc: 0.8788 - val_loss: 0.5973 - val_acc: 0.8584\n",
      "\n",
      "Epoch 00206: val_loss did not improve from 0.53798\n",
      "Epoch 207/250\n",
      "218/218 [==============================] - 76s 349ms/step - loss: 0.4835 - acc: 0.8806 - val_loss: 0.5707 - val_acc: 0.8604\n",
      "\n",
      "Epoch 00207: val_loss did not improve from 0.53798\n",
      "Epoch 208/250\n",
      "218/218 [==============================] - 53s 244ms/step - loss: 0.4746 - acc: 0.8831 - val_loss: 0.5100 - val_acc: 0.8774\n",
      "\n",
      "Epoch 00208: val_loss improved from 0.53798 to 0.50997, saving model to ./model/208-0.5100.h5\n",
      "Epoch 209/250\n",
      "218/218 [==============================] - 16s 72ms/step - loss: 0.4702 - acc: 0.8844 - val_loss: 0.5665 - val_acc: 0.8642\n",
      "\n",
      "Epoch 00209: val_loss did not improve from 0.50997\n",
      "Epoch 210/250\n",
      "218/218 [==============================] - 15s 67ms/step - loss: 0.4715 - acc: 0.8850 - val_loss: 0.5946 - val_acc: 0.8545\n",
      "\n",
      "Epoch 00210: val_loss did not improve from 0.50997\n",
      "Epoch 211/250\n",
      "218/218 [==============================] - 15s 70ms/step - loss: 0.4748 - acc: 0.8829 - val_loss: 0.5626 - val_acc: 0.8648\n",
      "\n",
      "Epoch 00211: val_loss did not improve from 0.50997\n",
      "Epoch 212/250\n",
      "218/218 [==============================] - 15s 69ms/step - loss: 0.4760 - acc: 0.8801 - val_loss: 0.5702 - val_acc: 0.8618\n",
      "\n",
      "Epoch 00212: val_loss did not improve from 0.50997\n",
      "Epoch 213/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "218/218 [==============================] - 15s 70ms/step - loss: 0.4746 - acc: 0.8820 - val_loss: 0.6248 - val_acc: 0.8505\n",
      "\n",
      "Epoch 00213: val_loss did not improve from 0.50997\n",
      "Epoch 214/250\n",
      "218/218 [==============================] - 16s 73ms/step - loss: 0.4698 - acc: 0.8813 - val_loss: 0.5725 - val_acc: 0.8640\n",
      "\n",
      "Epoch 00214: val_loss did not improve from 0.50997\n",
      "Epoch 215/250\n",
      "218/218 [==============================] - 15s 70ms/step - loss: 0.4658 - acc: 0.8833 - val_loss: 0.5910 - val_acc: 0.8543\n",
      "\n",
      "Epoch 00215: val_loss did not improve from 0.50997\n",
      "Epoch 216/250\n",
      "218/218 [==============================] - 15s 70ms/step - loss: 0.4671 - acc: 0.8833 - val_loss: 0.5863 - val_acc: 0.8557\n",
      "\n",
      "Epoch 00216: val_loss did not improve from 0.50997\n",
      "Epoch 217/250\n",
      "218/218 [==============================] - 15s 70ms/step - loss: 0.4691 - acc: 0.8828 - val_loss: 0.5640 - val_acc: 0.8611\n",
      "\n",
      "Epoch 00217: val_loss did not improve from 0.50997\n",
      "Epoch 218/250\n",
      "218/218 [==============================] - 15s 69ms/step - loss: 0.4672 - acc: 0.8826 - val_loss: 0.5450 - val_acc: 0.8667\n",
      "\n",
      "Epoch 00218: val_loss did not improve from 0.50997\n",
      "Epoch 219/250\n",
      "218/218 [==============================] - 15s 70ms/step - loss: 0.4669 - acc: 0.8836 - val_loss: 0.5403 - val_acc: 0.8689\n",
      "\n",
      "Epoch 00219: val_loss did not improve from 0.50997\n",
      "Epoch 220/250\n",
      "218/218 [==============================] - 15s 68ms/step - loss: 0.4624 - acc: 0.8879 - val_loss: 0.5153 - val_acc: 0.8744\n",
      "\n",
      "Epoch 00220: val_loss did not improve from 0.50997\n",
      "Epoch 221/250\n",
      "218/218 [==============================] - 15s 70ms/step - loss: 0.4674 - acc: 0.8844 - val_loss: 0.5842 - val_acc: 0.8518\n",
      "\n",
      "Epoch 00221: val_loss did not improve from 0.50997\n",
      "Epoch 222/250\n",
      "218/218 [==============================] - 15s 68ms/step - loss: 0.4645 - acc: 0.8852 - val_loss: 0.5695 - val_acc: 0.8577\n",
      "\n",
      "Epoch 00222: val_loss did not improve from 0.50997\n",
      "Epoch 223/250\n",
      "218/218 [==============================] - 15s 69ms/step - loss: 0.4597 - acc: 0.8856 - val_loss: 0.5588 - val_acc: 0.8611\n",
      "\n",
      "Epoch 00223: val_loss did not improve from 0.50997\n",
      "Epoch 224/250\n",
      "218/218 [==============================] - 15s 70ms/step - loss: 0.4646 - acc: 0.8812 - val_loss: 0.5464 - val_acc: 0.8622\n",
      "\n",
      "Epoch 00224: val_loss did not improve from 0.50997\n",
      "Epoch 225/250\n",
      "218/218 [==============================] - 15s 71ms/step - loss: 0.4555 - acc: 0.8866 - val_loss: 0.5458 - val_acc: 0.8673\n",
      "\n",
      "Epoch 00225: val_loss did not improve from 0.50997\n",
      "Epoch 226/250\n",
      "218/218 [==============================] - 15s 69ms/step - loss: 0.4627 - acc: 0.8840 - val_loss: 0.5832 - val_acc: 0.8558\n",
      "\n",
      "Epoch 00226: val_loss did not improve from 0.50997\n",
      "Epoch 227/250\n",
      "218/218 [==============================] - 15s 71ms/step - loss: 0.4608 - acc: 0.8841 - val_loss: 0.5287 - val_acc: 0.8709\n",
      "\n",
      "Epoch 00227: val_loss did not improve from 0.50997\n",
      "Epoch 228/250\n",
      "218/218 [==============================] - 15s 70ms/step - loss: 0.4602 - acc: 0.8847 - val_loss: 0.5259 - val_acc: 0.8729\n",
      "\n",
      "Epoch 00228: val_loss did not improve from 0.50997\n",
      "Epoch 229/250\n",
      "218/218 [==============================] - 16s 72ms/step - loss: 0.4596 - acc: 0.8836 - val_loss: 0.5778 - val_acc: 0.8562\n",
      "\n",
      "Epoch 00229: val_loss did not improve from 0.50997\n",
      "Epoch 230/250\n",
      "218/218 [==============================] - 16s 71ms/step - loss: 0.4570 - acc: 0.8847 - val_loss: 0.5365 - val_acc: 0.8692\n",
      "\n",
      "Epoch 00230: val_loss did not improve from 0.50997\n",
      "Epoch 231/250\n",
      "218/218 [==============================] - 15s 70ms/step - loss: 0.4630 - acc: 0.8831 - val_loss: 0.5550 - val_acc: 0.8613\n",
      "\n",
      "Epoch 00231: val_loss did not improve from 0.50997\n",
      "Epoch 232/250\n",
      "218/218 [==============================] - 15s 71ms/step - loss: 0.4567 - acc: 0.8840 - val_loss: 0.5925 - val_acc: 0.8541\n",
      "\n",
      "Epoch 00232: val_loss did not improve from 0.50997\n",
      "Epoch 233/250\n",
      "218/218 [==============================] - 15s 68ms/step - loss: 0.4553 - acc: 0.8849 - val_loss: 0.5274 - val_acc: 0.8711\n",
      "\n",
      "Epoch 00233: val_loss did not improve from 0.50997\n",
      "Epoch 234/250\n",
      "218/218 [==============================] - 15s 71ms/step - loss: 0.4546 - acc: 0.8849 - val_loss: 0.5451 - val_acc: 0.8662\n",
      "\n",
      "Epoch 00234: val_loss did not improve from 0.50997\n",
      "Epoch 235/250\n",
      "218/218 [==============================] - 15s 71ms/step - loss: 0.4517 - acc: 0.8863 - val_loss: 0.5674 - val_acc: 0.8612\n",
      "\n",
      "Epoch 00235: val_loss did not improve from 0.50997\n",
      "Epoch 236/250\n",
      "218/218 [==============================] - 15s 69ms/step - loss: 0.4565 - acc: 0.8842 - val_loss: 0.5629 - val_acc: 0.8577\n",
      "\n",
      "Epoch 00236: val_loss did not improve from 0.50997\n",
      "Epoch 237/250\n",
      "218/218 [==============================] - 15s 67ms/step - loss: 0.4547 - acc: 0.8847 - val_loss: 0.5407 - val_acc: 0.8699\n",
      "\n",
      "Epoch 00237: val_loss did not improve from 0.50997\n",
      "Epoch 238/250\n",
      "218/218 [==============================] - 15s 69ms/step - loss: 0.4529 - acc: 0.8837 - val_loss: 0.5287 - val_acc: 0.8708\n",
      "\n",
      "Epoch 00238: val_loss did not improve from 0.50997\n",
      "Epoch 239/250\n",
      "218/218 [==============================] - 15s 69ms/step - loss: 0.4559 - acc: 0.8837 - val_loss: 0.5608 - val_acc: 0.8611\n",
      "\n",
      "Epoch 00239: val_loss did not improve from 0.50997\n",
      "Epoch 240/250\n",
      "218/218 [==============================] - 15s 67ms/step - loss: 0.4596 - acc: 0.8823 - val_loss: 0.5562 - val_acc: 0.8590\n",
      "\n",
      "Epoch 00240: val_loss did not improve from 0.50997\n",
      "Epoch 241/250\n",
      "218/218 [==============================] - 16s 72ms/step - loss: 0.4480 - acc: 0.8867 - val_loss: 0.5472 - val_acc: 0.8623\n",
      "\n",
      "Epoch 00241: val_loss did not improve from 0.50997\n",
      "Epoch 242/250\n",
      "218/218 [==============================] - 16s 71ms/step - loss: 0.4535 - acc: 0.8858 - val_loss: 0.5705 - val_acc: 0.8590\n",
      "\n",
      "Epoch 00242: val_loss did not improve from 0.50997\n",
      "Epoch 243/250\n",
      "218/218 [==============================] - 15s 69ms/step - loss: 0.4540 - acc: 0.8845 - val_loss: 0.5816 - val_acc: 0.8601\n",
      "\n",
      "Epoch 00243: val_loss did not improve from 0.50997\n",
      "Epoch 244/250\n",
      "218/218 [==============================] - 15s 70ms/step - loss: 0.4532 - acc: 0.8850 - val_loss: 0.6235 - val_acc: 0.8501\n",
      "\n",
      "Epoch 00244: val_loss did not improve from 0.50997\n",
      "Epoch 245/250\n",
      "218/218 [==============================] - 15s 67ms/step - loss: 0.4474 - acc: 0.8869 - val_loss: 0.5828 - val_acc: 0.8544\n",
      "\n",
      "Epoch 00245: val_loss did not improve from 0.50997\n",
      "Epoch 246/250\n",
      "218/218 [==============================] - 15s 68ms/step - loss: 0.4548 - acc: 0.8858 - val_loss: 0.4985 - val_acc: 0.8768\n",
      "\n",
      "Epoch 00246: val_loss improved from 0.50997 to 0.49850, saving model to ./model/246-0.4985.h5\n",
      "Epoch 247/250\n",
      "218/218 [==============================] - 16s 71ms/step - loss: 0.4484 - acc: 0.8860 - val_loss: 0.5668 - val_acc: 0.8602\n",
      "\n",
      "Epoch 00247: val_loss did not improve from 0.49850\n",
      "Epoch 248/250\n",
      "218/218 [==============================] - 15s 70ms/step - loss: 0.4525 - acc: 0.8837 - val_loss: 0.5123 - val_acc: 0.8722\n",
      "\n",
      "Epoch 00248: val_loss did not improve from 0.49850\n",
      "Epoch 249/250\n",
      "218/218 [==============================] - 15s 69ms/step - loss: 0.4443 - acc: 0.8869 - val_loss: 0.5312 - val_acc: 0.8688\n",
      "\n",
      "Epoch 00249: val_loss did not improve from 0.49850\n",
      "Epoch 250/250\n",
      "218/218 [==============================] - 15s 69ms/step - loss: 0.4481 - acc: 0.8859 - val_loss: 0.5650 - val_acc: 0.8632\n",
      "\n",
      "Epoch 00250: val_loss did not improve from 0.49850\n"
     ]
    }
   ],
   "source": [
    "# training\n",
    "batch_size = 128\n",
    "opt_rms = keras.optimizers.rmsprop(lr=0.001, decay=1e-6)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "             optimizer=opt_rms,\n",
    "             metrics=['accuracy'])\n",
    "history = model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),\n",
    "                    steps_per_epoch=x_train.shape[0] // batch_size,epochs=250,\n",
    "                    verbose=1,validation_data=(x_test,y_test),callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save to disk\n",
    "# save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
    "# model_name = '60165060_trained_model.h5'\n",
    "\n",
    "# if not os.path.isdir(save_dir):\n",
    "#     os.makedirs(save_dir)\n",
    "# model_path = os.path.join(save_dir, model_name)\n",
    "# model.save(model_path)\n",
    "# print('Saved trained model at %s ' % model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# y_loss에 학습셋으로 실험 결과의 오차 값을 저장\n",
    "y_loss = history.history['val_loss']\n",
    "\n",
    "# y_acc에 학습셋으로 측정한 정확도의 값을 저장\n",
    "y_acc = history.history['acc']\n",
    "\n",
    "# x값을 지정하고 정확도를 파랑색으로, 오차를 빨강색으로 표시\n",
    "x_len = np.arange(len(y_acc))\n",
    "plt.plot(x_len, y_loss, \"o\", c=\"red\", markersize=3)\n",
    "plt.plot(x_len, y_acc, \"o\", c=\"blue\", markersize=3)\n",
    "\n",
    "plt.show()\n",
    "# loss: 1.5541 - acc: 0.4574\n",
    "# 그냥 baseline 그대로 돌리면 loss 1.4783 acc 0.5014\n",
    "# loss: 0.4548 - acc: 0.8858 - val_loss: 0.4985 - val_acc: 0.8768 확실히 성능 개선됨\n",
    "\n",
    "# lr 달리 줬더니 그림이 저렇게 나옴....."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12000/12000 [==============================] - 2s 128us/step\n",
      "\n",
      "Test result: 86.317 loss: 0.565\n"
     ]
    }
   ],
   "source": [
    "#testing\n",
    "scores = model.evaluate(x_test, y_test, batch_size=128, verbose=1)\n",
    "print('\\nTest result: %.3f loss: %.3f' % (scores[1]*100,scores[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "p36",
   "language": "python",
   "name": "p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
